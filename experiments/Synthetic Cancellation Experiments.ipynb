{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f386cc35-faad-49f1-a414-67bed86f795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e1dfae-581e-414d-b2d4-64f711de0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import captum\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from captum.metrics import infidelity, infidelity_perturb_func_decorator\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from txai.datasets import SyntheticCancellationDataset\n",
    "from txai.experiments import (\n",
    "    GenericExperimenter,\n",
    "    TabularInfidelityMetric,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb77464",
   "metadata": {},
   "source": [
    "## Experiment Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d4f558-ace1-49d0-be00-a9e7c7f65f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTINUOUS_DIM = 8\n",
    "TOTAL_DIM = CONTINUOUS_DIM * 2\n",
    "DATA_SEEDS = list(range(42, 42 + 5))\n",
    "NETWORK_SEEDS = list(range(101, 101 + 5*5, 5))\n",
    "NUM_EPOCHS = 2000\n",
    "Experiment = namedtuple('Experiment', ['name', 'continuous_dim', 'cancellation_likelihood', 'nn_dims', 'activation_fun'])\n",
    "EXPERIMENTS_MANUAL = [\n",
    "    Experiment(name=\"relu-2-dim\", continuous_dim=2, cancellation_likelihood=0.3, nn_dims=None, activation_fun=nn.ReLU),\n",
    "    Experiment(name=\"relu-3-dim\", continuous_dim=3, cancellation_likelihood=0.25, nn_dims=None, activation_fun=nn.ReLU),\n",
    "    Experiment(name=\"relu-4-dim\", continuous_dim=4, cancellation_likelihood=0.2, nn_dims=None, activation_fun=nn.ReLU),\n",
    "    Experiment(name=\"relu-5-dim\", continuous_dim=5, cancellation_likelihood=0.15, nn_dims=None, activation_fun=nn.ReLU),\n",
    "    Experiment(name=\"relu-100-dim\", continuous_dim=100, cancellation_likelihood=0.05, nn_dims=None, activation_fun=nn.ReLU),\n",
    "    Experiment(name=\"relu-1000-dim\", continuous_dim=1000, cancellation_likelihood=0.01, nn_dims=None, activation_fun=nn.ReLU),\n",
    "]\n",
    "EXPERIMENTS = [\n",
    "    Experiment(name=\"relu-2-dim\", continuous_dim=2, cancellation_likelihood=0.3, nn_dims=[4, 16, 16, 1], activation_fun=nn.ReLU),\n",
    "    Experiment(name=\"relu-3-dim\", continuous_dim=3, cancellation_likelihood=0.25, nn_dims=[6, 24, 24, 1], activation_fun=nn.ReLU),\n",
    "    Experiment(name=\"relu-4-dim\", continuous_dim=4, cancellation_likelihood=0.2, nn_dims=[8, 32, 32, 1], activation_fun=nn.ReLU),\n",
    "    Experiment(name=\"relu-5-dim\", continuous_dim=5, cancellation_likelihood=0.15, nn_dims=[10, 40, 40, 1], activation_fun=nn.ReLU),\n",
    "    Experiment(name=\"gelu-2-dim\", continuous_dim=2, cancellation_likelihood=0.3, nn_dims=[4, 16, 16, 1], activation_fun=nn.GELU),\n",
    "    Experiment(name=\"gelu-3-dim\", continuous_dim=3, cancellation_likelihood=0.25, nn_dims=[6, 24, 24, 1], activation_fun=nn.GELU),\n",
    "    Experiment(name=\"gelu-4-dim\", continuous_dim=4, cancellation_likelihood=0.2, nn_dims=[8, 32, 32, 1], activation_fun=nn.GELU),\n",
    "    Experiment(name=\"gelu-5-dim\", continuous_dim=5, cancellation_likelihood=0.15, nn_dims=[10, 40, 40, 1], activation_fun=nn.GELU),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f71350-0db3-41c4-b0fa-c5404a616a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_nn(nn_dims, activation_fun):    \n",
    "    layers = []\n",
    "    for i in range(1, len(nn_dims)):\n",
    "        in_dim, out_dim = nn_dims[i-1], nn_dims[i]\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        layers.append(activation_fun())\n",
    "    # Remove the last activation layer (regression problem)\n",
    "    layers = layers[:-1]\n",
    "    \n",
    "    return nn.Sequential(*layers).to(DEVICE)\n",
    "\n",
    "def train_nn(model, train_dl):\n",
    "    loss_fun = nn.MSELoss()\n",
    "    opt = torch.optim.AdamW(model.parameters())\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in tqdm(range(NUM_EPOCHS), leave=False):\n",
    "        total_loss = 0\n",
    "        for i, (x, y) in list(enumerate(train_dl)):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = loss_fun(out.squeeze(-1), y.float())\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        losses.append(total_loss)\n",
    "\n",
    "def eval_nn(model, test_dataset):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = model(torch.stack(test_dataset.samples).to(DEVICE)).detach().numpy()\n",
    "    labels = torch.tensor(test_dataset.labels).unsqueeze(-1).detach().numpy()\n",
    "\n",
    "    rmse = mean_squared_error(\n",
    "        labels, predictions, multioutput='uniform_average', squared=False\n",
    "    )\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35161d9-0ea3-4205-8fbb-062d9667dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_cafe_const(c, total_dim):\n",
    "    def explain_cafe(model, x):\n",
    "        cafe = CafeUpgradeExplainer(model, c=c)\n",
    "        split_x = torch.split(x, 50)\n",
    "        all_attrs = []\n",
    "        for x_part in split_x:\n",
    "            ((s_feat_plus, s_feat_minus), _) = cafe.attribute(x_part, ref=torch.zeros(total_dim), target=0)\n",
    "            attribution = s_feat_plus - s_feat_minus\n",
    "            all_attrs.append(attribution)\n",
    "        attributions = torch.cat(tuple(all_attrs))\n",
    "        return attributions\n",
    "\n",
    "    return explain_cafe\n",
    "\n",
    "def explain_cafe_const_smooth(c, total_dim):\n",
    "    def explain_cafe(model, x):\n",
    "        cafe = CafeSmoothExplainer(model, c=c)\n",
    "        ((s_feat_plus, s_feat_minus), _) = cafe.attribute(x, ref=torch.zeros(1, total_dim), target=0)\n",
    "        attribution = s_feat_plus - s_feat_minus\n",
    "        return attribution\n",
    "\n",
    "    return explain_cafe\n",
    "\n",
    "def explain_ixg(model, x):\n",
    "    with warnings.catch_warnings():\n",
    "        # Suppress annoying warnings about setting activation hooks & time consuming operations\n",
    "        # IMPORTANT: Re-enable warnings when changing code to ensure things still work\n",
    "        warnings.simplefilter('ignore')\n",
    "        ixg_captum = captum.attr.InputXGradient(model)\n",
    "        attribution = ixg_captum.attribute(x)\n",
    "    return attribution\n",
    "\n",
    "def explain_lrp(model, x):\n",
    "    with warnings.catch_warnings():\n",
    "        # Suppress annoying warnings about setting activation hooks & time consuming operations\n",
    "        # IMPORTANT: Re-enable warnings when changing code to ensure things still work\n",
    "        warnings.simplefilter('ignore')\n",
    "        lrp_captum = captum.attr.LRP(model)\n",
    "        attribution = lrp_captum.attribute(x)\n",
    "    return attribution\n",
    "\n",
    "def explain_dl_const(total_dim, multiply_by_inputs=True):\n",
    "    def explain_dl(model, x):\n",
    "        with warnings.catch_warnings():\n",
    "            # Suppress annoying warnings about setting activation hooks & time consuming operations\n",
    "            # IMPORTANT: Re-enable warnings when changing code to ensure things still work\n",
    "            warnings.simplefilter('ignore')\n",
    "            dl_captum = captum.attr.DeepLift(model, multiply_by_inputs=multiply_by_inputs)\n",
    "            attribution, delta = dl_captum.attribute(x, baselines=torch.zeros(1, total_dim), return_convergence_delta=True)\n",
    "        return attribution\n",
    "\n",
    "    return explain_dl\n",
    "\n",
    "def explain_ig_const(total_dim, multiply_by_inputs=True):\n",
    "    def explain_ig(model, x):\n",
    "        with warnings.catch_warnings():\n",
    "            # Suppress annoying warnings about setting activation hooks & time consuming operations\n",
    "            # IMPORTANT: Re-enable warnings when changing code to ensure things still work\n",
    "            warnings.simplefilter('ignore')\n",
    "            ig_captum = captum.attr.IntegratedGradients(model, multiply_by_inputs=multiply_by_inputs)\n",
    "            attribution, delta = ig_captum.attribute(x, baselines=torch.zeros(1, total_dim), return_convergence_delta=True)\n",
    "        return attribution\n",
    "\n",
    "    return explain_ig\n",
    "\n",
    "def explain_sg_const(multiply_by_inputs=True):\n",
    "    def explain_sg(model, x):\n",
    "        with warnings.catch_warnings():\n",
    "            # Suppress annoying warnings about setting activation hooks & time consuming operations\n",
    "            # IMPORTANT: Re-enable warnings when changing code to ensure things still work\n",
    "            warnings.simplefilter('ignore')\n",
    "            nt_captum = captum.attr.NoiseTunnel(captum.attr.Saliency(model))\n",
    "            attribution = nt_captum.attribute(x)\n",
    "            if multiply_by_inputs:\n",
    "                attribution *= x\n",
    "        return attribution\n",
    "\n",
    "    return explain_sg\n",
    "\n",
    "def explain_gs_const(total_dim, multiply_by_inputs=True):\n",
    "    def explain_gs(model, x):\n",
    "        with warnings.catch_warnings():\n",
    "            # Suppress annoying warnings about setting activation hooks & time consuming operations\n",
    "            # IMPORTANT: Re-enable warnings when changing code to ensure things still work\n",
    "            warnings.simplefilter('ignore')\n",
    "            gs_captum = captum.attr.GradientShap(model, multiply_by_inputs=multiply_by_inputs)\n",
    "            attribution = gs_captum.attribute(x, baselines=torch.zeros(1, total_dim))\n",
    "        return attribution\n",
    "\n",
    "    return explain_gs\n",
    "\n",
    "def explain_ks_const(total_dim, multiply_by_inputs=False):\n",
    "    def explain_ks(model, x):\n",
    "        with warnings.catch_warnings():\n",
    "            # Suppress annoying warnings about setting activation hooks & time consuming operations\n",
    "            # IMPORTANT: Re-enable warnings when changing code to ensure things still work\n",
    "            warnings.simplefilter('ignore')\n",
    "            ks_captum = captum.attr.KernelShap(model)\n",
    "            attribution = ks_captum.attribute(x, baselines=torch.zeros(1, total_dim))\n",
    "            if multiply_by_inputs:\n",
    "                attribution *= x\n",
    "        return attribution\n",
    "\n",
    "    return explain_ks\n",
    "\n",
    "def explain_svs_const(total_dim, multiply_by_inputs=False):\n",
    "    def explain_svs(model, x):\n",
    "        with warnings.catch_warnings():\n",
    "            # Suppress annoying warnings about setting activation hooks & time consuming operations\n",
    "            # IMPORTANT: Re-enable warnings when changing code to ensure things still work\n",
    "            warnings.simplefilter('ignore')\n",
    "            svs_captum = captum.attr.ShapleyValueSampling(model)\n",
    "            attribution = svs_captum.attribute(x, baselines=torch.zeros(1, total_dim), target=0)\n",
    "            if multiply_by_inputs:\n",
    "                attribution *= x\n",
    "        return attribution\n",
    "\n",
    "    return explain_svs\n",
    "\n",
    "def explain_lime_const(total_dim, multiply_by_inputs=False):\n",
    "    def explain_lime(model, x):\n",
    "        with warnings.catch_warnings():\n",
    "            # Suppress annoying warnings about setting activation hooks & time consuming operations\n",
    "            # IMPORTANT: Re-enable warnings when changing code to ensure things still work\n",
    "            warnings.simplefilter('ignore')\n",
    "            lime_captum = captum.attr.Lime(model)\n",
    "            split_x = torch.split(x, 50)\n",
    "            all_attrs = []\n",
    "            for x_part in split_x:\n",
    "                attribution = lime_captum.attribute(x_part, baselines=torch.zeros(1, total_dim))\n",
    "                if multiply_by_inputs:\n",
    "                    attribution *= x_part\n",
    "                all_attrs.append(attribution)\n",
    "            attributions = torch.cat(tuple(all_attrs))\n",
    "        return attributions\n",
    "\n",
    "    return explain_lime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bc86a-bfa6-45d3-9e46-550e3f09cf75",
   "metadata": {},
   "source": [
    "## Manually Constructed Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed8eb2d-de8d-4c67-8877-a58ed761aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_manual_relu_nn(continuous_dim, weights):\n",
    "    # First linear layer returns output in the form [x1, x2, -x1, -x2, c1, c2]\n",
    "    # where xs are the continuous features and cs are the cancellation features\n",
    "    l1 = nn.Linear(continuous_dim * 2, continuous_dim * 3, bias=False)\n",
    "    w = torch.zeros((continuous_dim * 3, continuous_dim * 2))\n",
    "    w[0:continuous_dim, 0:continuous_dim] = torch.diag_embed(weights)\n",
    "    w[continuous_dim:continuous_dim * 2, 0:continuous_dim] = torch.diag_embed(-weights)\n",
    "    w[continuous_dim * 2:continuous_dim * 3, continuous_dim:continuous_dim * 2] = torch.eye(continuous_dim)\n",
    "    l1.weight = nn.Parameter(w)\n",
    "\n",
    "    # Second layer returns output in the form [ReLU(x1) - 100 * c1, ReLU(x2) - 100 * c2, -ReLU(x1) - 100, -ReLU(x2) * (1 - c2)]\n",
    "    l2 = nn.Linear(continuous_dim * 3, continuous_dim * 2, bias=False)\n",
    "    w = torch.zeros((continuous_dim * 2, continuous_dim * 3))\n",
    "    w[0:continuous_dim, 0:continuous_dim] = torch.eye(continuous_dim)\n",
    "    w[0:continuous_dim, continuous_dim*2:continuous_dim*3] = -50 * torch.eye(continuous_dim)\n",
    "    w[continuous_dim:continuous_dim*2, continuous_dim:continuous_dim*2] = torch.eye(continuous_dim)\n",
    "    w[continuous_dim:continuous_dim*2, continuous_dim*2:continuous_dim*3] = -50 * torch.eye(continuous_dim)\n",
    "    l2.weight = nn.Parameter(w)\n",
    "\n",
    "    # Finally, the output layer computes\n",
    "    l3 = nn.Linear(continuous_dim * 2, 1, bias=False)\n",
    "    w = torch.zeros((1, continuous_dim * 2))\n",
    "    w[0, 0:continuous_dim] = 1.\n",
    "    w[0, continuous_dim:continuous_dim*2] = -1.\n",
    "    l3.weight = nn.Parameter(w)\n",
    "    return nn.Sequential(l1, nn.ReLU(), l2, nn.ReLU(), l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc62865-84b8-44a7-a0a0-47dedfb5f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_id, experiment in enumerate(EXPERIMENTS_MANUAL):\n",
    "    experiment_name = experiment.name\n",
    "    if 'relu' not in experiment_name:\n",
    "        # Only ReLU supported for manually consturcted networks\n",
    "        continue\n",
    "    continuous_dim = experiment.continuous_dim\n",
    "    total_dim = continuous_dim * 2\n",
    "    cancellation_likelihood = experiment.cancellation_likelihood\n",
    "    activation_fun = experiment.activation_fun\n",
    "    nn_dims = [continuous_dim * 2, continuous_dim * 3, continuous_dim * 2, 1]\n",
    "    adjusted_experiment = deepcopy(experiment)\n",
    "    adjusted_experiment = adjusted_experiment._replace(nn_dims=nn_dims)\n",
    "\n",
    "    print(f\"———————————————[ Running experiment #{experiment_id + 1}/{len(EXPERIMENTS_MANUAL)} ]———————————————\")\n",
    "    print(f\"———————[ Experiment parameters ]———————\")\n",
    "    print(adjusted_experiment)\n",
    "    print()\n",
    "\n",
    "    # Note: Whether to multiply attributions returned by each method by the inputs was\n",
    "    #       chosen according to which of the two variants exhibited better performance.\n",
    "    methods = [\n",
    "        (\"Gradient x Input\", explain_ixg),\n",
    "        (\"LRP\", explain_lrp),\n",
    "        (\"DeepLIFT Rescale\", explain_dl_const(total_dim)),\n",
    "        # (\"Integrated Gradients\", explain_ig_const(total_dim)),\n",
    "        # (\"SmoothGrad (Multiplicative)\", explain_sg_const()),\n",
    "        (\"Gradient SHAP\", explain_gs_const(total_dim)),\n",
    "        (\"Kernel SHAP\", explain_ks_const(total_dim)),\n",
    "        (\"Shapley Value Sampling\", explain_svs_const(total_dim)),\n",
    "        # (\"LIME\", explain_lime_const(total_dim)),\n",
    "        # (\"CAFE (c = 0.0)\", explain_cafe_const(0.0, total_dim)),\n",
    "        # (\"CAFE (c = 0.25)\", explain_cafe_const(0.25, total_dim)),\n",
    "        # (\"CAFE (c = 0.5)\", explain_cafe_const(0.5, total_dim)),\n",
    "        # (\"CAFE (c = 0.75)\", explain_cafe_const(0.75, total_dim)),\n",
    "        # (\"CAFE (c = 1.0)\", explain_cafe_const(1.0, total_dim)),\n",
    "        (\"Cafe Smooth (c = 0.0)\", explain_cafe_const_smooth(0.0, total_dim)),\n",
    "        (\"Cafe Smooth (c = 0.25)\", explain_cafe_const_smooth(0.25, total_dim)),\n",
    "        (\"Cafe Smooth (c = 0.5)\", explain_cafe_const_smooth(0.5, total_dim)),\n",
    "        (\"Cafe Smooth (c = 0.75)\", explain_cafe_const_smooth(0.75, total_dim)),\n",
    "        (\"Cafe Smooth (c = 1.0)\", explain_cafe_const_smooth(1.0, total_dim)),\n",
    "    ]\n",
    "\n",
    "    result_headers = [\"Validation RMSE\", \"Test RMSE\"] + [m[0] for m in methods]\n",
    "    results = []\n",
    "    runtime_headers = [f\"{m[0]} Runtime\" for m in methods]\n",
    "    runtimes = []\n",
    "    \n",
    "    for data_seed, nn_start_seed in tqdm(list(zip(DATA_SEEDS, NETWORK_SEEDS)), leave=False):\n",
    "        current_results = []\n",
    "        current_runtimes = []\n",
    "        \n",
    "        dataset = SyntheticCancellationDataset.generate(\n",
    "            10000,\n",
    "            seed=data_seed,\n",
    "            continuous_dim=continuous_dim,\n",
    "            cancellation_likelihood=cancellation_likelihood,\n",
    "            standard_dev=10.,\n",
    "            weight_range=(-1., 1.),\n",
    "        )\n",
    "\n",
    "        train_dataset, tmp_val_dataset = dataset.split(0.6)\n",
    "        val_dataset, test_dataset = tmp_val_dataset.split(0.5)\n",
    "        train_dl = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        selected_model = None\n",
    "        best_rmse = np.inf\n",
    "        for nn_seed in tqdm(range(nn_start_seed, nn_start_seed + 5), leave=False):\n",
    "            torch.manual_seed(nn_seed)\n",
    "            model = construct_manual_relu_nn(continuous_dim, dataset.weights)\n",
    "            val_rmse = eval_nn(model, val_dataset)\n",
    "            if val_rmse < best_rmse:\n",
    "                selected_model = model\n",
    "                best_rmse = val_rmse\n",
    "        model = selected_model\n",
    "        \n",
    "        val_rmse = eval_nn(model, val_dataset)\n",
    "        current_results.append(val_rmse)\n",
    "        test_rmse = eval_nn(model, test_dataset)\n",
    "        current_results.append(test_rmse)\n",
    "\n",
    "        model.eval()\n",
    "        model.zero_grad()\n",
    "\n",
    "        for method_name, method_fun in tqdm(methods, leave=False):\n",
    "            torch.manual_seed(data_seed)\n",
    "            np.random.seed(data_seed)\n",
    "            exp_model = deepcopy(model)\n",
    "            start_time = time.time()\n",
    "            all_attributions = method_fun(exp_model, torch.stack(test_dataset.samples).to(DEVICE))\n",
    "            end_time = time.time()\n",
    "            current_runtimes.append(end_time - start_time)\n",
    "            attributions_error = mean_squared_error(\n",
    "                torch.stack(test_dataset.ground_truth_attributions).detach().numpy(),\n",
    "                all_attributions.detach().numpy(),\n",
    "                multioutput='uniform_average',\n",
    "                squared=False\n",
    "            )\n",
    "            current_results.append(attributions_error)\n",
    "\n",
    "        results.append(current_results)\n",
    "        runtimes.append(current_runtimes)\n",
    "\n",
    "    results = np.array(results)\n",
    "    means = results.mean(axis=0)\n",
    "    stds = results.std(axis=0)\n",
    "\n",
    "    print(f\"———————[ Experiment results ]———————\")\n",
    "    for header, mean, std in zip(result_headers, means, stds):\n",
    "        print(f\"{header}: {'{:.2f}'.format(round(mean, 2))}±{'{:.3f}'.format(round(std, 3))}\")\n",
    "    print()\n",
    "\n",
    "    results = np.array(runtimes)\n",
    "    means = results.mean(axis=0)\n",
    "    stds = results.std(axis=0)\n",
    "\n",
    "    print(f\"———————[ Experiment runtimes ]———————\")\n",
    "    for header, mean, std in zip(runtime_headers, means, stds):\n",
    "        print(f\"{header}: {'{:.3f}'.format(round(mean, 3))}±{'{:.4f}'.format(round(std, 4))}\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3d3d6-1289-4250-bb96-27c9a8440b7b",
   "metadata": {},
   "source": [
    "## Ground-Truth Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241b41c-d90f-4482-83da-43f39c940b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_id, experiment in enumerate(EXPERIMENTS):\n",
    "    experiment_name = experiment.name\n",
    "    continuous_dim = experiment.continuous_dim\n",
    "    total_dim = continuous_dim * 2\n",
    "    cancellation_likelihood = experiment.cancellation_likelihood\n",
    "    nn_dims = experiment.nn_dims\n",
    "    activation_fun = experiment.activation_fun\n",
    "\n",
    "    print(f\"———————————————[ Running experiment #{experiment_id + 1}/{len(EXPERIMENTS)} ]———————————————\")\n",
    "    print(f\"———————[ Experiment parameters ]———————\")\n",
    "    print(experiment)\n",
    "    print()\n",
    "\n",
    "    # Note: Whether to multiply attributions returned by each method by the inputs was\n",
    "    #       chosen according to which of the two variants exhibited better performance.\n",
    "    methods = [\n",
    "        (\"Gradient x Input\", explain_ixg),\n",
    "        (\"LRP\", explain_lrp),\n",
    "        (\"DeepLIFT Rescale\", explain_dl_const(total_dim)),\n",
    "        # (\"Integrated Gradients\", explain_ig_const(total_dim)),\n",
    "        # (\"SmoothGrad (Multiplicative)\", explain_sg_const()),\n",
    "        (\"Gradient SHAP\", explain_gs_const(total_dim)),\n",
    "        (\"Kernel SHAP\", explain_ks_const(total_dim)),\n",
    "        (\"Shapley Value Sampling\", explain_svs_const(total_dim)),\n",
    "        # (\"LIME\", explain_lime_const(total_dim)),\n",
    "        (\"CAFE (c = 0.0)\", explain_cafe_const(0.0, total_dim)),\n",
    "        (\"CAFE (c = 0.25)\", explain_cafe_const(0.25, total_dim)),\n",
    "        (\"CAFE (c = 0.5)\", explain_cafe_const(0.5, total_dim)),\n",
    "        (\"CAFE (c = 0.75)\", explain_cafe_const(0.75, total_dim)),\n",
    "        (\"CAFE (c = 1.0)\", explain_cafe_const(1.0, total_dim)),\n",
    "        (\"Cafe Smooth (c = 0.0)\", explain_cafe_const_smooth(0.0, total_dim)),\n",
    "        (\"Cafe Smooth (c = 0.25)\", explain_cafe_const_smooth(0.25, total_dim)),\n",
    "        (\"Cafe Smooth (c = 0.5)\", explain_cafe_const_smooth(0.5, total_dim)),\n",
    "        (\"Cafe Smooth (c = 0.75)\", explain_cafe_const_smooth(0.75, total_dim)),\n",
    "        (\"Cafe Smooth (c = 1.0)\", explain_cafe_const_smooth(1.0, total_dim)),\n",
    "    ]\n",
    "\n",
    "    result_headers = [\"Validation RMSE\", \"Test RMSE\"] + [m[0] for m in methods]\n",
    "    results = []\n",
    "    \n",
    "    for data_seed, nn_start_seed in tqdm(list(zip(DATA_SEEDS, NETWORK_SEEDS)), leave=False):\n",
    "        current_results = []\n",
    "        \n",
    "        dataset = SyntheticCancellationDataset.generate(\n",
    "            10000,\n",
    "            seed=data_seed,\n",
    "            continuous_dim=continuous_dim,\n",
    "            cancellation_likelihood=cancellation_likelihood,\n",
    "            standard_dev=10.,\n",
    "            weight_range=(-1., 1.),\n",
    "        )\n",
    "\n",
    "        train_dataset, tmp_val_dataset = dataset.split(0.6)\n",
    "        val_dataset, test_dataset = tmp_val_dataset.split(0.5)\n",
    "        train_dl = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        selected_model = None\n",
    "        best_rmse = np.inf\n",
    "        for nn_seed in tqdm(range(nn_start_seed, nn_start_seed + 5), leave=False):\n",
    "            torch.manual_seed(nn_seed)\n",
    "            model = construct_nn(nn_dims, activation_fun)\n",
    "            checkpoint_path = f\"models/{experiment_name}-d{data_seed}-nn{nn_seed}.pth\"\n",
    "            if os.path.isfile(checkpoint_path):\n",
    "                # Load the saved checkpoint\n",
    "                model.load_state_dict(torch.load(checkpoint_path))\n",
    "            else:\n",
    "                # Train and save the model\n",
    "                train_nn(model, train_dl)\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "            val_rmse = eval_nn(model, val_dataset)\n",
    "            if val_rmse < best_rmse:\n",
    "                selected_model = model\n",
    "                best_rmse = val_rmse\n",
    "        model = selected_model\n",
    "        \n",
    "        val_rmse = eval_nn(model, val_dataset)\n",
    "        current_results.append(val_rmse)\n",
    "        test_rmse = eval_nn(model, test_dataset)\n",
    "        current_results.append(test_rmse)\n",
    "\n",
    "        model.eval()\n",
    "        model.zero_grad()\n",
    "\n",
    "        for method_name, method_fun in tqdm(methods, leave=False):\n",
    "            torch.manual_seed(data_seed)\n",
    "            np.random.seed(data_seed)\n",
    "            exp_model = deepcopy(model)\n",
    "            all_attributions = method_fun(exp_model, torch.stack(test_dataset.samples).to(DEVICE))\n",
    "            attributions_error = mean_squared_error(\n",
    "                torch.stack(test_dataset.ground_truth_attributions).detach().numpy(),\n",
    "                all_attributions.detach().numpy(),\n",
    "                multioutput='uniform_average',\n",
    "                squared=False\n",
    "            )\n",
    "            current_results.append(attributions_error)\n",
    "\n",
    "        results.append(current_results)\n",
    "\n",
    "    results = np.array(results)\n",
    "    means = results.mean(axis=0)\n",
    "    stds = results.std(axis=0)\n",
    "\n",
    "    print(f\"———————[ Experiment results ]———————\")\n",
    "    for header, mean, std in zip(result_headers, means, stds):\n",
    "        print(f\"{header}: {'{:.2f}'.format(round(mean, 2))}±{'{:.3f}'.format(round(std, 3))}\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904ed2d-f755-472e-8fb0-99c48425ba63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
